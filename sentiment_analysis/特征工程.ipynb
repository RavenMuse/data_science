{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本特征工程\n",
    "0. 数据源：舆情事件（新闻、微信）、词向量（搜狗新闻）\n",
    "1. 获取样本事件ID、事件名、情感分数\n",
    "2. 对事件名进行分词、清洗\n",
    "3. 建立词向量特征（n * maxlen * k ）\n",
    "4. 建立词性特征 （n * maxlen * 1）\n",
    "5. 建立词长特征 （n * maxlen * 1）\n",
    "6. 建立负向词位置、数量特征 （n * 2）\n",
    "7. 建立正向词位置、数量特征 （n * 2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:58:45.391631Z",
     "start_time": "2019-11-21T09:58:45.371687Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import bz2\n",
    "import pymysql\n",
    "import jieba.posseg as pseg\n",
    "import collections\n",
    "from functools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:56:57.657929Z",
     "start_time": "2019-11-21T09:56:57.637872Z"
    }
   },
   "outputs": [],
   "source": [
    "# dev\n",
    "MYSQL_IP = '10.248.224.3'\n",
    "MYSQL_PORT = 11202\n",
    "MYSQL_USER = 'bigdata'\n",
    "MYSQL_PSW = '3jHj8qid0ZxXn18'\n",
    "MYSQL_DB = 'dmall_public_opinion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T10:14:51.074192Z",
     "start_time": "2019-11-21T10:14:51.034211Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_by_sql(sql):\n",
    "    \"\"\"\n",
    "    通过sql获取数据\n",
    "    :param sql:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    db = pymysql.connect(host=MYSQL_IP, port=MYSQL_PORT, user=MYSQL_USER, passwd=MYSQL_PSW,\n",
    "                         db=MYSQL_DB, charset='utf8')\n",
    "    return pd.read_sql(sql, con=db)\n",
    "\n",
    "def add_user_words(user_dict):\n",
    "    \"\"\"\n",
    "    添加用户词典\n",
    "    :param user_dict:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for word, tag in user_dict.items():\n",
    "        jieba.add_word(word, freq=1000000, tag=tag)\n",
    "\n",
    "def cut_word(text):\n",
    "    \"\"\"\n",
    "    文本分词\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    words_flag=[]\n",
    "    words_length=[]\n",
    "    for item in pseg.cut(text,HMM=False):  \n",
    "        word=item.word\n",
    "        flag=item.flag\n",
    "        if word in stop_dict or flag in ['x','eng']:\n",
    "            continue\n",
    "        words.append(word)\n",
    "        words_flag.append(flag)\n",
    "        words_length.append(len(word))\n",
    "    return words,words_flag,words_length,len(words)\n",
    "\n",
    "def get_sentiment(row):\n",
    "    \"\"\"\n",
    "    获得舆情情感值\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    scores=np.array([row[\"negative_score\"],row[\"other_score\"],row[\"positive_score\"]])\n",
    "    return scores.argmax()-1\n",
    "\n",
    "def get_dict(data,col_name):\n",
    "    \"\"\"\n",
    "    获取数据集词典\n",
    "    :param sql:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    all_value = reduce(lambda x, y: x + y,data[col_name])\n",
    "    freq_sorted = sorted(collections.Counter(all_value).items(),\n",
    "                               key=lambda item: item[1],\n",
    "                               reverse=True)\n",
    "    res_list = list(map(lambda item: item[0], freq_sorted))\n",
    "    \n",
    "    return res_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:56:53.368117Z",
     "start_time": "2019-11-21T09:56:53.358116Z"
    }
   },
   "outputs": [],
   "source": [
    "# 全局变量\n",
    "## 分词分析停用词、用户词典\n",
    "stop_dict = []\n",
    "user_dict = {\"肿么了\": \"user\"}\n",
    "add_user_words(user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:55:59.413879Z",
     "start_time": "2019-11-21T09:55:57.303352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24700 entries, 0 to 24699\n",
      "Data columns (total 3 columns):\n",
      "event_id      24700 non-null object\n",
      "event_name    24700 non-null object\n",
      "sentiment     24700 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 579.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# 0. 数据源：舆情事件（新闻、微信）\n",
    "sql=\"select event_id,event_name,sentiment from event_sentiment where media_type=1 and sentiment is not null\"\n",
    "\n",
    "# 1. 获取样本事件ID、事件名、情感分数\n",
    "data=get_data_by_sql(sql)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T09:57:46.175070Z",
     "start_time": "2019-11-21T09:57:43.045272Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. 对事件名进行分词、清洗\n",
    "# 3. 建立词向量特征（n * maxlen * k ）\n",
    "# 4. 建立词性特征 （n * maxlen * 1）\n",
    "# 5. 建立词长特征 （n * maxlen * 1）\n",
    "words_with_flag=data.event_name.apply(lambda item:cut_word(item))\n",
    "data['words']=words_with_flag.apply(lambda item:item[0])\n",
    "data['words_flag']=words_with_flag.apply(lambda item:item[1])\n",
    "data['words_length']=words_with_flag.apply(lambda item:item[2])\n",
    "data['length']=words_with_flag.apply(lambda item:item[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 建立负向词位置、数量特征 （n * 2）\n",
    "# 7. 建立正向词位置、数量特征 （n * 2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T10:00:00.695790Z",
     "start_time": "2019-11-21T09:58:53.371200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date length : 24700\n",
      "total words count : 28533\n"
     ]
    }
   ],
   "source": [
    "# 获取词典\n",
    "print(\"date length : %s\"%len(data))\n",
    "words_dict_list = get_dict(data,'words')\n",
    "print(\"total words count : %s\"%len(words_dict_list))\n",
    "# 保存词典\n",
    "np.save('words_dict',words_dict_list)\n",
    "\n",
    "# 获取词频字典\n",
    "words_flag_dict_list = get_dict(data,'words_flag')\n",
    "print(\"total words_flag count : %s\"%len(words_flag_dict_list))\n",
    "# 保存词频字典\n",
    "np.save('words_flag_dict',words_flag_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T10:01:48.869560Z",
     "start_time": "2019-11-21T10:01:27.820775Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集中词映射为词典编号\n",
    "data['words_index'] = data.words.apply(\n",
    "    lambda item:list(map( lambda word: words_dict_list.index(word),item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T10:18:56.417228Z",
     "start_time": "2019-11-21T10:18:56.237261Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集中词映射为词典编号\n",
    "data['words_flag_index'] = data.words_flag.apply(\n",
    "    lambda item:list(map( lambda flag: words_flag_dict_list.index(flag),item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T10:18:58.877086Z",
     "start_time": "2019-11-21T10:18:58.307142Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存样本\n",
    "data[['event_id','words','words_index','words_flag','words_flag_index','words_length','length','sentiment']].to_csv(\"data_featured.csv\", header=True, encoding='utf_8_sig', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 根据样本词集合载入词向量\n",
    "# 文件格式（词+向量）： word 1 2 3 4\n",
    "# 词向量词典\n",
    "embedding_matrix = np.zeros((len(words_dict_list), 300))\n",
    "word_vector_count = 0\n",
    "is_first_line = True\n",
    "file = bz2.open('sgns.sogounews.bigram-char.bz2', mode='r')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    if is_first_line:\n",
    "        print(\"word count : %s\" % values[0])\n",
    "        print(\"vector size : %s\" % values[1])\n",
    "        is_first_line = False\n",
    "        continue\n",
    "    word = values[0].decode('utf-8')\n",
    "    if word in words_dict_list:\n",
    "        index = words_dict_list.index(word)\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_matrix[index] = coefs\n",
    "        word_vector_count = word_vector_count + 1\n",
    "\n",
    "file.close()\n",
    "print('Found %s word vectors.' % word_vector_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embedding_matrix',embedding_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
