{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:33:46.196820Z",
     "start_time": "2019-11-21T02:33:46.176787Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import keras\n",
    "from functools import *\n",
    "import collections\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.sequence import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "0. 载入数据集,建立模型参数\n",
    "1. 获取数据集词典，并限制词量\n",
    "2. 数据集中词映射为词典编号\n",
    "3. 根据样本词集合载入词向量\n",
    "4. 对短句填充至最大句长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:36:05.586848Z",
     "start_time": "2019-11-21T02:36:05.576830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14914"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:36:21.445927Z",
     "start_time": "2019-11-21T02:36:21.255986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# 0. 载入数据集,建立模型参数\n",
    "data = pd.read_csv(\"data_featured.csv\", encoding='utf_8_sig').head(1000)\n",
    "# 数据集中最大句子长度\n",
    "print(data.length.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:33:50.486570Z",
     "start_time": "2019-11-21T02:33:50.466539Z"
    }
   },
   "outputs": [],
   "source": [
    "model_config={\n",
    "    'max_length':60, # 最大句长\n",
    "    'max_words':25000, # 最大词量\n",
    "    'embedding_dim':300, # 词向量长度\n",
    "    'model_path':'baseline_sentiment_model.h5',\n",
    "    'words_dict_path':'words_dict.npy'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:36:49.334348Z",
     "start_time": "2019-11-21T02:36:48.724394Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. 读取字典\n",
    "words_dict_list =np.load(model_config['words_dict_path']).tolist()\n",
    "\n",
    "\n",
    "# 2.数据集中词映射为词典编号\n",
    "data['words_index'] = data.words.apply(\n",
    "    lambda item:list(map( lambda word: words_dict_list.index(word),eval(item))))\n",
    "data['words_flag'] = data.words_flag.apply(lambda item: eval(item))\n",
    "data['words_length'] = data.words_length.apply(lambda item: eval(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T07:59:08.794523Z",
     "start_time": "2019-11-20T07:54:53.959308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count : b'365113'\n",
      "vector size : b'300'\n",
      "Found 21109 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 3. 根据样本词集合载入词向量\n",
    "# 文件格式（词+向量）： word 1 2 3 4\n",
    "# 词向量词典\n",
    "embedding_matrix = np.zeros(\n",
    "    (model_config['max_words'], model_config['embedding_dim']))\n",
    "word_vector_count = 0\n",
    "is_first_line = True\n",
    "file = bz2.open('sgns.sogounews.bigram-char.bz2', mode='r')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    if is_first_line:\n",
    "        print(\"word count : %s\" % values[0])\n",
    "        print(\"vector size : %s\" % values[1])\n",
    "        is_first_line = False\n",
    "        continue\n",
    "    word = values[0].decode('utf-8')\n",
    "    if word in words_dict_list:\n",
    "        index = words_dict_list.index(word)\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_matrix[index] = coefs\n",
    "        word_vector_count = word_vector_count + 1\n",
    "\n",
    "file.close()\n",
    "print('Found %s word vectors.' % word_vector_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:36:53.214073Z",
     "start_time": "2019-11-21T02:36:53.174134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of words_data tensor: (1000, 60)\n"
     ]
    }
   ],
   "source": [
    "# 4. 对短句填充至最大句长\n",
    "ids=data['event_id'].values\n",
    "words_data = pad_sequences(data['words_index'].values, maxlen=model_config['max_length'],value=model_config['max_words'])\n",
    "print('Shape of words_data tensor:', words_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:34:23.944608Z",
     "start_time": "2019-11-21T02:34:23.134677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 60, 300)           7500000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 60, 300)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 60, 128)           38528     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 7,818,595\n",
      "Trainable params: 318,467\n",
      "Non-trainable params: 7,500,128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 2.建立模型\n",
    "input_tensor = Input(shape=(None,))\n",
    "embedding_layer=Embedding(model_config['max_words'], model_config['embedding_dim'] , input_length=model_config['max_length'])(input_tensor)\n",
    "dropout_layer = SpatialDropout1D(0.12)(embedding_layer)\n",
    "#flatten_layer = Flatten()(dropout_layer)\n",
    "X_sentence = TimeDistributed(Dense(128, activation='relu'))(dropout_layer)\n",
    "L_sentence = Bidirectional(LSTM(128))(X_sentence)\n",
    "L_sentence = Dropout(0.5)(L_sentence)\n",
    "\n",
    "L_sentence = Dense(64, activation='relu')(L_sentence)\n",
    "L_sentence = BatchNormalization()(L_sentence)\n",
    "L_sentence = Dropout(0.5)(L_sentence)\n",
    "output_tensor = Dense(3, activation='sigmoid')(L_sentence)\n",
    "model = Model(input_tensor, output_tensor)\n",
    "# Embedding层载入词典索引为行号的词向量,Embedding层不参与训练\n",
    "# model.layers[1].set_weights([embedding_matrix])\n",
    "model.layers[1].trainable = False\n",
    "print(model.summary())\n",
    "# 模型优化\n",
    "model.compile(optimizer='adamax', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:35:30.500775Z",
     "start_time": "2019-11-21T02:35:25.081113Z"
    }
   },
   "outputs": [],
   "source": [
    "# 模型载入\n",
    "model.load_weights('baseline_sentiment_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:37:40.345559Z",
     "start_time": "2019-11-21T02:37:38.845610Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6. 模型预测\n",
    "def get_sentiment(scores):\n",
    "    index= scores.argmax()\n",
    "    if index==2:\n",
    "        return -1\n",
    "    return index\n",
    "predict_raw= model.predict(words_data)\n",
    "test_predict=pd.DataFrame()\n",
    "test_predict['event_id']= ids\n",
    "test_predict['predict_sentiment']=list(map(lambda item: get_sentiment(item),predict_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:37:45.375290Z",
     "start_time": "2019-11-21T02:37:45.335263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>predict_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E8D6FD90FCB51DF95C30AB20D16A4DB3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>288B9B734B201EFEA4E9B0D6FFC0A67A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BFAAF179CAA5C190991E5ED7EF30EDE5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17EB6E8B8B31ED7A7C5677F3E9574DF8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8D25659863A4986EBD7875D17E5A5314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>760471EDEE98B8E6DE5D50DB2C92A73E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>14694C45627DCE843FB4E698D899D059</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2214483E09FEA823992104B43EA74D5C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>E7DF7D097581A0009A9DC3B2CF26D85E</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2BE43B359DA2F1B2C94EA95E5E90FA93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             event_id  predict_sentiment\n",
       "0    E8D6FD90FCB51DF95C30AB20D16A4DB3                  0\n",
       "1    288B9B734B201EFEA4E9B0D6FFC0A67A                  0\n",
       "2    BFAAF179CAA5C190991E5ED7EF30EDE5                  0\n",
       "3    17EB6E8B8B31ED7A7C5677F3E9574DF8                  0\n",
       "4    8D25659863A4986EBD7875D17E5A5314                  1\n",
       "..                                ...                ...\n",
       "995  760471EDEE98B8E6DE5D50DB2C92A73E                  1\n",
       "996  14694C45627DCE843FB4E698D899D059                  0\n",
       "997  2214483E09FEA823992104B43EA74D5C                  0\n",
       "998  E7DF7D097581A0009A9DC3B2CF26D85E                  0\n",
       "999  2BE43B359DA2F1B2C94EA95E5E90FA93                  0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(data.sentiment, data.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T02:39:23.067247Z",
     "start_time": "2019-11-21T02:39:23.027221Z"
    }
   },
   "outputs": [],
   "source": [
    "res=pd.merge(data,test_predict,on='event_id')\n",
    "#.to_csv(\"predict_sentiment.csv\", header=True, encoding='utf_8_sig', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T03:17:53.233489Z",
     "start_time": "2019-11-21T03:17:53.203491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy : 0.568\n"
     ]
    }
   ],
   "source": [
    "# 准确率\n",
    "print(\"accurancy : %s\"%(len(res[res.sentiment==res.predict_sentiment])/len(res)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 349.79999999999995,
   "position": {
    "height": "40px",
    "left": "797px",
    "right": "20px",
    "top": "86px",
    "width": "576.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
